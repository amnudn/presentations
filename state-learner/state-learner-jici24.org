* Notes/todo :noexport:
- [ ] Start with motivation for super learner.

* Setting :noexport:
Remember to exceture (C-c C-c) the following line:
#+PROPERTY: header-args:R :async :results output verbatim  :exports results  :session *R* :cache yes

* Super learning with right-censored data
** Setting and motivation

Construct a targeted estimator of a low-dimensional target parameter such as the
average treatment effect.

\vfill

Available data are right-censored.

\vfill

Requires estimation of high-dimensional nuisance parameters such as the
conditional survival and censoring mechanism.

\vfill

Use super learning to alleviate the need to fully pre-specify estimators of the
nuisance parameters.

\vfill

\color{bblue}Challenge\color{black}: Existing super learners restrict which
learners can be included in the library or require pre-specification of an
estimator of the censoring mechanism.

** Super learning[fn:1]

\small

#+begin_export latex
Need to estimate the parameter
\begin{equation*}
  f = \argmin_{f \in \mathcal{F}} P{[L(f, \blank)]}, 
\end{equation*}
using the data set \( \mathcal{D}_n = \{O_1, \dots, O_n\} \).
#+end_export

\vfill

- Learner :: algorithm \( a \) that produces estimates, \( \mathcal{D}_n \mapsto
  a(\mathcal{D}_n) = \hat f_n \)
- Library :: collection of learners, \( \mathcal{A} = \{a_1, a_2, \dots, a_M
  \}\)
- Loss function :: \( L \colon \mathcal{F} \times \mathcal{O} \rightarrow \R \)

\vfill

Combine learners from the library into a new learner with performance almost as
good as the best of them.

\vfill

\color{bblue}Regression example\color{black}: \( L(f, O) =(f(X) -Y)^2 \),
library consiting of the learners =lm=, =glm=, =glmnet=, =rfsrc=, ...

[fn:1] \cite{stone1974cross,geisser1975predictive,wolpert1992stacked,breiman1996stacked,van2007super}

** Discrete super learner

\small

- Learner :: algorithm \( a \) that produces estimates, \( \mathcal{D}_n \mapsto
  a(\mathcal{D}_n) = \hat f_n \)
- Library :: collection of learners, \( \mathcal{A} = \{a_1, a_2, \dots, a_M \}
  \)
- Loss function :: \( L \colon \mathcal{F} \times \mathcal{O} \rightarrow \R \)

\vfill

#+begin_export latex
The discrete super learning is the data-adaptive learner
\begin{equation*}
  \hat{a}_n = \argmin_{a\in\mathcal A}\hat{R}_n(a; L),
\end{equation*}
where
\begin{equation*}
  \hat{R}_n(a; L) =
  \frac{1}{K}\sum_{k=1}^{K}
  \frac{1}{| \mathcal{D}_n^{k} |}\sum_{O_i \in \mathcal{D}_n^{k}}
  L
  {
    \left(
      a{ (\mathcal{D}_n^{-k})}
      , O_i
    \right)
  },
  \quad \text{with} \quad
  \mathcal{D}_n^{-k} = \mathcal{D}_n \setminus \mathcal{D}_n^{k}.
\end{equation*}
#+end_export

** Right-censored data

\small

*** Notation
- \color{black}\(X\) :: vector of baseline covariates
- \( T \) :: time to event variable, \( T > 0 \)
- \( C \) :: censoring time, \( C > 0 \)
- \( Q \) :: distribution of the data of interest \( (X, T) \sim Q \)
- \color{darkgray}\(\tilde T \)\color{black} :: censored time to event variable, \(
  \tilde T = \min(T, C) \)
- \color{darkgray}\( \Delta \)\color{black} :: binary event indicator, \( \Delta =
  \1{\{T \leq C\}} \)
- \color{darkgray}\( P \)\color{black} :: distribution of the observed data, \( O =
  (X, \tilde T, \Delta) \sim P \)

\hfill

We use \color{bblue} \( \Lambda \) \color{black} and \color{bblue}\(\Gamma\)
\color{black} to denote the conditional cumulative hazard functions for \( T \)
and \( C \), i.e.,
#+begin_export latex
\begin{equation*}
  \Lambda(\diff t \mid x) = Q(T \in \diff t \mid T \geq t, X=x).
\end{equation*}
#+end_export

Assuming \( T \independent C \mid X \) and positivity so that $\Lambda$ and
$\Gamma$ are identifiable from \( P \) on some \( [0,\tau] \).

** Super learning and targeted learning

#+begin_export latex
\small Parameters of interest could be
\begin{align*}
  \Psi_t(Q) &  = Q(T > t),
  \\
  \Psi_t(Q)
  % &= \int \left\{ Q(T > t \mid X=x, A=1) - Q(T > t \mid X=x, A=0) \right\}
  % Q(\diff x)
    &=
      \E_Q{\left[ Q(T > t \mid X, A=1) - Q(T > t \mid X, A=0) \right]},
      \quad \text{or} 
  \\
  \Psi_t(Q) & = \E_Q{\left[ \E_Q[T \wedge t \mid X, A=1] - \E_Q[T \wedge t \mid
              X, A=0]   \right]}.
\end{align*}
#+end_export

\vfill

Can be expressed using $\Lambda$ so we they are identifiable from \( P \in
\mathcal{P} \) and we may write \( \tilde{\Psi}_t(P) = \Psi_t(Q(P)) \).

\vfill

The efficient influence function for \( \tilde{\Psi}_t \) can be indexed by
$(\Lambda, \Gamma)$ or $(\Lambda, \Gamma, \pi)$, where $\pi$ is the propensity
model.

\vfill

Goal is to use super learning to estimate the parameters $(\Lambda, \Gamma)$ so
that we can construct a targeted estimator of \( \tilde{\Psi}_t \).

** Super learning with right-censored data

# \small

- \( Q \) :: distribution of \( (X, T) \sim Q \)
- \color{darkgray}\( P \)\color{black} :: distribution of \( O = (X, \tilde T,
  \Delta) \sim P \)

\vfill

Typically want to estimate a feature of \( Q \) such as $\Lambda$.

\vfill

Data set \( \mathcal{D}_n = \{O_1, \dots, O_n\} \), with \( O = (X, \tilde T,
\Delta) \) from some \( P \) available.

\vfill


#+begin_export latex
Super learning relies on calculating
\begin{equation*}
  \hat{R}_n(a; L) =
  \frac{1}{K}\sum_{k=1}^{K}
  \frac{1}{| \mathcal{D}_n^{k} |}\sum_{O_i \in \mathcal{D}_n^{k}}
  L
  {
    \left(
      a{ (\mathcal{D}_n^{-k})}
      , O_i
    \right)
  },
  \quad \text{with} \quad
  \mathcal{D}_n^{-k} = \mathcal{D}_n \setminus \mathcal{D}_n^{k}.
\end{equation*}
#+end_export

\pause
\vfill

- \( a{ (\mathcal{D}_n^{-k})} \) :: Training learners in censored data
  \checkmark
- 
- \( L(a{ (\mathcal{D}_n^{-k})} , O_i) \) :: Evaluating fitted learners in
  censored data ?


* Existing approaches

** The negative log-likelihood is unsuited for super learning

*** overlay block 
:PROPERTIES:
:BEAMER_act: <1>
:BEAMER_env: onlyenv
:END:


Commonly used loss function for survival data is the negative (partial)
log-likelihood.

\vfill

Factorizes into separate components for $\Lambda$ and $\Gamma$.

\vfill

Unsuited for general use due to point masses.


*** overlay block 
:PROPERTIES:
:BEAMER_act: <2>
:BEAMER_env: onlyenv
:END:

#+ATTR_LATEX: :width 0.9\textwidth
[[./sl-hold-out-sample.pdf]]


** Existing approaches
\small
*** \normalsize Negative log-likelihood loss function \footnotesize (e.g., \cite{polley2011-sl-cens})
Requires discrete time or modeling of a Lebesgue hazard function.

\hfill

*** \normalsize Pseudo-observations \footnotesize (e.g., \cite{sachs2019ensemble})
Requires pre-specification of an estimator of the censoring mechanism.

\hfill

*** \normalsize IPCW \footnotesize (e.g., \cite{graf1999assessment,hothorn2006survival,gerds2006consistent,gonzalez2021stacked})
Requires pre-specification of an estimator of the censoring mechanism.

\hfill

*** \normalsize Iterative IPCW \footnotesize (\cite{westling2021inference,han2021inverse})
Theoretical guarantees?


* Proposal: The state learner
** Proposal: Model the states of the /observed/ data

*** placeholder
:PROPERTIES:
:BEAMER_act: <1>
:BEAMER_env: onlyenv
:END:

**** \centering \color{white} \( (X, T) \sim Q \)
#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 4 :width 8
plot.new()
#+END_SRC

#+RESULTS[(2024-01-29 15:44:53) 456b2027ae0e06c2fd1b6ce146971f0886962a4e]:
[[file:/tmp/babel-q4X6IR/figure-vDga9Y.pdf]]

*** ideal
:PROPERTIES:
:BEAMER_act: <2>
:BEAMER_env: onlyenv
:END:

**** \centering \( (X, T) \sim Q \)
#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 4 :width 8
  library(prodlim)
  try(setwd("~/Documents/presentations/state-learner/"))
  nTrans <- 1
  stateLabels = c("Initial","Event")
  crHist <- Hist(time = 1:nTrans, event = list(from = rep("1", nTrans), to = stateLabels[-1]))
  plot(crHist,stateLabels = stateLabels,arrowLabels = FALSE, color = "white")
#+END_SRC

#+RESULTS[(2024-01-29 15:44:53) a881f0fa7af0b8752288dde93fee800947ed427a]:
[[file:/tmp/babel-q4X6IR/figure-8pG1XI.pdf]]

*** observed
:PROPERTIES:
:BEAMER_act: <3>
:BEAMER_env: onlyenv
:END:

**** \centering \color{darkgray}\( (X, \tilde T, \Delta) \sim P \)\color{black}
#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 4 :width 8
nTrans <- 2
stateLabels = c("Initial","Event", "Censored")
crHist <- Hist(time = 1:nTrans, event = list(from = rep("1", nTrans), to = stateLabels[-1]))
plot(crHist,stateLabels = stateLabels,arrowLabels = FALSE)
#+END_SRC

#+RESULTS[(2024-01-29 15:44:53) 482338ffd116268c93f6a3be61c1f7ab4c82876f]:
[[file:/tmp/babel-q4X6IR/figure-NRCHP8.pdf]]


** Conditional state-occupation probabilities for observed data
\small

#+begin_export latex
Define
% Record the observed data as \( O = (X, \{\eta(t) : t \geq 0\}) \), where
\begin{equation*}
  \eta(t) = \1{\{\tilde{T} \leq t, \Delta = 1\}} + 2 \, \1{\{\tilde{T} \leq t,
    \Delta = 0\}}
  \in \{0, 1, 2\},
\end{equation*}
and
\begin{equation*}
  F(t, j, x) = P(\eta(t) = j \mid X=x), \quad \text{for all } t \geq 0,\; j
  \in \{0, 1, 2\}, \; x \in \R^d.
\end{equation*}
#+end_export

\vfill

*** multi state 1                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:

\centering \color{bblue}\( O = (X, \tilde T , \Delta) \)\color{black} 

#+ATTR_LATEX: :width 0.9\textwidth
[[./multi-state-data-1.pdf]]

*** multi state 2                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:

\centering \color{bblue}\( O = (X, \{\eta(t) : t \geq 0\}) \)\color{black}

#+ATTR_LATEX: :width 0.9\textwidth
[[./multi-state-data-3.pdf]]

** The state learner

\small

#+begin_export latex
\begin{equation*}
  F(t, j, x) = P(\eta(t) = j \mid X=x), \quad \text{for all } t \geq 0,\; j
  \in \{0, 1, 2\}, \; x \in \R^d.
\end{equation*}
#+end_export

\vfill

The state learner is a super learner of \( F \).

\vfill

#+begin_export latex
Performance can be evaluated using, e.g., the integrated Brier score
\( \bar{B}_{\tau}(F, O) = \int_0^{\tau} B_t(F, O) \diff t \), where
\begin{equation*}
  B_t(F, O) = \sum_{j=0}^{2} (F(t, j, X) - \eta(t))^2.
\end{equation*}
#+end_export

\vfill

Loss function does not depend on unknown nuisance parameters.

\vfill

No modeling of Lebesgue hazards or densities required.

** Expressing \( F \) using $\Lambda$ and $\Gamma$

\small
# The conditional state-occupation probabilites

#+begin_export latex
\begin{equation*}
\begin{split}
F(t, 1, x)
& = P(\tilde{T} \leq t, \Delta=1 \mid X=x)
  = \int_0^t e^{-\Lambda(s \mid x) - \Gamma(s \mid x) }  \Lambda(\diff s \mid x),
\\
F(t, 2, x)
& = P(\tilde{T} \leq t, \Delta=0 \mid X=x)
  = \int_0^t e^{-\Lambda(s \mid x) - \Gamma(s \mid x) }  \Gamma(\diff s \mid x),
\\
F(t, 0, x)
&
  = P(\tilde{T} > t \mid X= x)
  = 1- F(t, 1, x) - F(t, 2, x).
\end{split}
\end{equation*}
#+end_export

\vspace{.5cm}

*** multi state 2                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:

\centering \(  \eta(t) : t \geq 0 \)

#+ATTR_LATEX: :width 0.9\textwidth
[[./multi-state-data-3.pdf]]

*** multi state system                                                :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 3 :width 6
nTrans <- 2
stateLabels = c("Initial","Event", "Censored")
crHist <- Hist(time = 1:nTrans, event = list(from = rep("1", nTrans), to = stateLabels[-1]))
plot(crHist,
     stateLabels = stateLabels,
     arrow2.label=paste(expression(Lambda)),
     arrow1.label=paste(expression(Gamma)),
     changeArrowLabelSide=c(FALSE,TRUE))
#+END_SRC

#+RESULTS[(2024-01-29 15:44:53) 38fab6b49e1ceb848c18f5dd7645a445cf8e9bf1]:
[[file:/tmp/babel-q4X6IR/figure-Rf0nt3.pdf]]

** Constructing a library for learning \( F \)

\small

#+begin_export latex
Given libraries \( \mathcal{A} \) and \( \mathcal{B} \) for learning $\Lambda$
and $\Gamma$, respectively,  define
\begin{equation*}
  \mathcal{F}(\mathcal{A}, \mathcal{B})
  = \{ \phi_{a, b} : a \in \mathcal{A}, b \in \mathcal{B}\},
\end{equation*}
where
\begin{align*}
  \phi_{a, b}(\mathcal{D}_n)(t,1,x) &= \int_0^t e^{-a(\mathcal{D}_n)(s \mid
    x) -
    b(\mathcal{D}_n)(s \mid x) }  a(\mathcal{D}_n)(\diff s \mid x),
  \\
  & \dots 
\end{align*}
#+end_export

\vfill

#+begin_export latex
Evaluate performance of
\( \phi_{a, b} \in \mathcal{F}(\mathcal{A}, \mathcal{B}) \) as
\begin{equation*}
  \hat{R}_n(\phi_{a, b}; \bar{B}_{\tau}) =
  \frac{1}{K}\sum_{k=1}^{K}
  \frac{1}{| \mathcal{D}_n^{k} |}\sum_{O_i \in \mathcal{D}_n^{k}}
  \int_0^{\tau} \sum_{j=0}^{2} 
  \left\{
    \phi_{a, b}(\mathcal{D}_n^{-k})(t,j, X_i) - \eta_i(t)
  \right\}^2 \diff t.
\end{equation*}
#+end_export

** Extension to competing risks setting
\small

# The state learner can be adapted to competing risk settings by adding a state:

#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 3 :width 9
  nTrans <- 3
  stateLabels = c("Initial","Cause 1","Cause 2", "Censored")
  crHist <- Hist(time = 1:nTrans, event = list(from = rep("1", nTrans), to = stateLabels[-1]))
  plot(crHist,
       stateLabels = stateLabels,
       arrow1.label=paste(expression(Lambda[1])),
       arrow2.label=paste(expression(Lambda[2])),
       arrow3.label=paste(expression(Gamma)),
       changeArrowLabelSide=c(TRUE, TRUE, FALSE))
#+END_SRC

#+RESULTS[(2024-01-29 15:44:53) e56a8511971bbf781238b297b342973c8f102a1f]:
[[file:/tmp/babel-q4X6IR/figure-1C6wEk.pdf]]

#+begin_export latex
\begin{equation*}
  \eta(t) = \1{\{\tilde{T} \leq t, \tilde{D} = 1\}} +
  2 \, \1{\{\tilde{T} \leq t, \tilde{D} = 2\}}
  +
  3 \, \1{\{\tilde{T} \leq t, \tilde{D} = 0\}}.
\end{equation*}
\begin{align*}
  F(t, 1, x)
  & = P(\tilde{T} \leq t, \tilde{D}=1 \mid X=x)
    = \int_0^t e^{-\Lambda_1(s \mid x) -\Lambda_2(s \mid x) - \Gamma(s \mid x) }  \Lambda_1(\diff s \mid x),
  \\
  & \dots
\end{align*}
\begin{equation*}
  \mathcal{F}(\mathcal{A}_1,\mathcal{A}_2, \mathcal{B})
  = \{ \phi_{a_1, a_2, b} : a_1 \in \mathcal{A}_1, a_2 \in \mathcal{A}_2, b \in \mathcal{B}\},
\end{equation*}
#+end_export

** Prototype[fn:github]

#+BEGIN_SRC R :exports none
  library(targets) 
  try(tar_source(here("~/Documents/phd/statelearner/R-code/functions/")))
#+END_SRC

#+RESULTS[(2024-01-29 16:50:56) 3a0ab8757a12b8a0a094196a6d27a54e14b3c58a]:


*** overlay block 
:PROPERTIES:
:BEAMER_act: <1>
:BEAMER_env: onlyenv
:END:



#+BEGIN_SRC R :exports both
library(riskRegression)
data(Melanoma, package="riskRegression")
setDT(Melanoma)
head(Melanoma)
#+END_SRC

#+RESULTS[(2024-01-29 16:51:57) 8edcc1af35203c1171b61fd7c9736a1bab865bed]:
:    time status                    event invasion ici      epicel       ulcer thick    sex age   logthick
: 1:   10      2       death.other.causes  level.1   2     present     present  6.76   Male  76  1.9110229
: 2:   30      2       death.other.causes  level.0   0 not present not present  0.65   Male  56 -0.4307829
: 3:   35      0                 censored  level.1   2 not present not present  1.34   Male  41  0.2926696
: 4:   99      2       death.other.causes  level.0   2 not present not present  2.90 Female  71  1.0647107
: 5:  185      1 death.malignant.melanoma  level.2   2     present     present 12.08   Male  52  2.4915512
: 6:  204      1 death.malignant.melanoma  level.2   2 not present     present  4.84   Male  28  1.5769147

*** overlay block 
:PROPERTIES:
:BEAMER_act: <2>
:BEAMER_env: onlyenv
:END:

#+BEGIN_SRC R :exports code
library(glmnet)
library(randomForestSRC)
lib <- list(
  cox = list(model = "cox", x_form = ~sex+age+logthick),
  cox_penalty = list(model = "GLMnet", x_form = ~invasion+ici+epicel+ulcer+sex+age+logthick),
  km = list(model = "cox", x_form = ~1),
  cox_strat = list(model = "cox", x_form = ~strata(epicel)),
  rf = list(model = "rfsrc", x_form = ~invasion+ici+epicel+ulcer+sex+age+logthick, ntree = 50)
)
#+END_SRC

#+RESULTS[(2024-01-29 16:53:58) d63c3740e65889cb734feecad729b36672005451]:

*** overlay block 
:PROPERTIES:
:BEAMER_act: <3>
:BEAMER_env: onlyenv
:END:

#+BEGIN_SRC R :exports code
set.seed(111)
sl = statelearner(learners = list(cause1 = lib,
                                  cause2 = lib,
                                  censor = lib),
                  data = Melanoma,
                  time = 5*365.25)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :exports both
sl$cv_fit
#+END_SRC

#+RESULTS[(2024-01-29 16:56:43) c3bd31dd70ae66e593592bb7c2695d9010e0864e]:
#+begin_example
        cause1      cause2      censor     loss b
  1:        rf          km         cox 239.6142 1
  2:        rf          km cox_penalty 239.8218 1
  3:        rf          km          km 239.8678 1
  4:        rf cox_penalty         cox 239.9478 1
  5:        rf          km          rf 239.9732 1
 ---                                             
121: cox_strat         cox          km 258.8383 1
122:        km         cox cox_penalty 259.0642 1
123: cox_strat         cox   cox_strat 259.0704 1
124:        km         cox          km 259.1725 1
125:        km         cox   cox_strat 259.3449 1
#+end_example


*** Footnotes
[fn:github] https://github.com/amnudn/statelearner

** Some theoretical results 

\small \pause

*** Strictly proper scoring rule
#+begin_export latex
We have that
\begin{equation*}
  F_P = \argmin_{F} P{[ \bar{B}_{\tau}(F, \blank)]},
\end{equation*}
for
\begin{equation*}
  F_P(t, j, x) = P(\eta(t) = j \mid X=x).
\end{equation*}
#+end_export

\hfill \pause

*** Finite sample guarantee[fn:cv-ref]

#+begin_export latex
For all $\delta > 0$ and $n \in \N$,
\begin{align*}
  \E_{P}{\left[ \Vert \hat{\phi}_n(\mathcal{D}_n^{-k}) - F_P \Vert_{P}^2 \right]}
  & \leq (1 + 2\delta)
    \E_{P}{\left[ \Vert \tilde{\phi}_n(\mathcal{D}_n^{-k}) - F_P \Vert_{P}^2 \right]}
  \\
  & \quad
    + (1+ \delta) 16   K \tau
    \left(
    13 + \frac{12}{\delta}
    \right)
    \frac{\log(1 + |\mathcal{F}_n|)}{n}.
\end{align*}
#+end_export

[fn:cv-ref] \cite{van2003unicv,van2006oracle}

** Proof of concept
\small

Wrongly assuming (completely) independent censoring can lead to poor performance
of IPCW based super learner.

#+ATTR_LATEX: :width .9\textwidth
[[./ipcw-fail.pdf]]

** Illustration on real data with competing risks[fn:data]

#+ATTR_LATEX: :width 1\textwidth
[[./zelefski-real-data.pdf]]

[fn:data] Data from a prostate cancer studied by \cite{kattan2000pretreatment}.

** Use with targeted learning

***  \centering \(  F \longleftrightarrow (\Lambda, \Gamma)  \)
#+begin_export latex
\begin{equation*}
  \Lambda(t \mid x) = \int_0^t \frac{F(\diff s, 1, x)}{F(s-, 0, x)} ,
  \quad \text{and} \quad
  \Gamma(t \mid x) = \int_0^t \frac{F(\diff s, 2, x)}{F(s-, 0, x)} .
\end{equation*}
#+end_export

*** 


The efficient influence function for \( \tilde{\Psi}_t \) can be indexed by
$(\Lambda, \Gamma)$ or \( F \).

*** 

The output from the state learner can be applied to construct a targeted
estimator.

** Second order remainder term

#+begin_export latex
Important property that
\begin{equation*}
  \text{Rem}(\hat{P}_n, P) = \Psi(P) - \Psi(\hat{P}_n) - P{[
    \psi(\blank, \hat{P}_n)]}
\end{equation*}
is of second order.
#+end_export

\vfill

#+begin_export latex
For survival problems, \( \text{Rem}(\hat{P}_n, P) \) is typically dominated
by terms of the form
\begin{equation*}  
  \E_P{\left[ \int_0^t \hat{w}(s, X) \hat{M}_1(s \mid X)  \hat{M}_2(\diff s \mid X)] \right]},
\end{equation*}
where \( \hat{M}_j \) is either \( [\Lambda - \hat{\Lambda}_n] \) or
\( [\Gamma - \hat{\Gamma}_n ]\).
#+end_export



** Second order remainder with the state learner

Second order property remains, in the sense that the remainder is dominated by
terms of the form,
#+begin_export latex
\begin{equation*}  
  \E_P{\left[ \int_0^t
      \hat{w}_*(s, X) [F - \hat{F}](s-, j, X)  [F- \hat{F}](\diff s, j', X)] \right]},
\end{equation*}
#+end_export

\vfill

Second order in terms of convergence rate.



* Discussion
** Ongoing work

Extension to continuous super learner. How to best construct a convex
combination?
#+begin_export latex
\begin{equation*}
  \sum_{(a, b)} \alpha_{a,b} \phi_{a, b},
  \quad \text{or} \quad 
  \phi_{\sum_a w_a a, \sum_b w_b b}, 
\end{equation*}
where
\begin{align*}
  \phi_{a, b}(\mathcal{D}_n)(t,1,x) &= \int_0^t e^{-a(\mathcal{D}_n)(s \mid
    x) -
    b(\mathcal{D}_n)(s \mid x) }  a(\mathcal{D}_n)(\diff s \mid x),
  \\
  & \dots 
\end{align*}
#+end_export


\vfill

Simulation study to assess effect on low-dimensional target parameters.

\vfill

Better implementation. Make more learners available.

** Discussion and open questions

Limitation that \( F \) is a feature of the observed data distribution.
Important whether we estimate \( F \) or \( (\Lambda, \Gamma) \) when the
parameter of interest is $\Psi \colon \mathcal{P} \rightarrow \R$?

\vfill

Second order remainder in terms of rates. Some double robustness property might
be lost. Relying too much on good estimation of all of \( F \)?

\vfill

Can we build or good risk prediction from the state learner?

\vfill

Will a targeted estimator based on the state learner be robust against or
sensitive to near positivity violations?

** Summary

- Aim to avoid the need to pre-specify a censoring model.
- Select a tuple (or triple) of learners \( (\Lambda, \Gamma) \) jointly optimal
  for predicting the states occupied by the observed data
- No need to estimate additional nuisance parameters in the hold-out sample.
- No need to model Lebesgue densities or hazards.
- Drawback that the state learner is tuned for the a feature of the observed
  data distribution.

* References
:PROPERTIES:
:UNNUMBERED: t
:END:
** References
# :PROPERTIES:
# :BEAMER_opt: allowframebreaks,label=
# :END:
\tiny \bibliography{./latex-settings/default-bib.bib}

* Setting :noexport:
Remember to exceture (C-c C-c) the following line:
#+PROPERTY: header-args:R :async :results output verbatim  :exports results  :session *R* :cache yes

* HEADER :noexport:
#+TITLE: The state learner \newline \normalsize a super learner for right-censored data
#+Author: Anders Munch \newline \small joint work with Thomas Gerds
#+Date: January 30, 2024

#+LANGUAGE:  en
#+OPTIONS:   H:2 num:t toc:nil ':t ^:t
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [smaller]
#+LaTeX_HEADER: \usepackage{natbib, dsfont, pgfpages, tikz,amssymb, amsmath,xcolor}
#+LaTeX_HEADER: \bibliographystyle{abbrvnat}
#+LaTeX_HEADER: \input{./latex-settings/standard-commands.tex}
#+BIBLIOGRAPHY: ./latex-settings/default-bib plain

#+LaTeX_HEADER: \definecolor{bblue}{rgb}{0.2,0.2,0.7}
#+LaTeX_HEADER: \usepackage{prodint}

# Beamer settins:
# #+LaTeX_HEADER: \usefonttheme[onlymath]{serif} 
#+LaTeX_HEADER: \setbeamertemplate{footline}[frame number]
#+LaTeX_HEADER: \beamertemplatenavigationsymbolsempty
#+LaTeX_HEADER: \usepackage{appendixnumberbeamer}
#+LaTeX_HEADER: \setbeamercolor{gray}{bg=white!90!black}
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+LATEX_HEADER: \setbeamertemplate{itemize items}{$\circ$}

# Setting size of code block
#+LaTeX_HEADER: \lstset{basicstyle=\ttfamily\footnotesize}
# Using when output of code is verbatim
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\footnotesize}

# Tweak footnotes
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
#+LATEX_HEADER:    \mathsection\or \mathparagraph\or \|\or \dagger\dagger
#+LATEX_HEADER:    \or \ddagger\ddagger \else\@ctrerr\fi}}
#+LATEX_HEADER: \makeatother
#+LATEX_HEADER: \renewcommand*{\thefootnote}{\fnsymbol{footnote}}
#+LATEX_HEADER: \AtBeginEnvironment{frame}{\setcounter{footnote}{0}}

