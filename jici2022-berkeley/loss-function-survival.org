* R setup and script                                               :noexport:
Remember to exceture (C-c C-c) the following line:
#+PROPERTY: header-args:R :async :results output verbatim  :exports results  :session *R* :cache yes

#+BEGIN_SRC R
  library(prodlim)
  library(survival)
  library(riskRegression)
  library(ggplot2)
  library(data.table)
  library(cowplot)
  library(latex2exp)
  library(here)
  setwd(here("jici2022-berkeley"))
#+END_SRC

#+RESULTS[(2022-08-31 12:41:44) 239ca2ef2e7195b64049fa98b5aefd48ef97a30d]:
: riskRegression version 2022.03.22
: data.table 1.14.2 using 4 threads (see ?getDTthreads).  Latest news: r-datatable.com
: here() starts at /home/amnudn/Documents/phd/presentations

#+BEGIN_SRC R
  set.seed(3)
  T <- rgamma(50, shape = 1)
  C <- rgamma(50, shape = 1)
  dt0 <- data.table(time = pmin(T, C), event = 1*(T<C))
  pd0 <- dt0[event == 1, .(time = sort(time))]
  pd0[["surv"]] <- 1-predictRisk(prodlim(Surv(time, event) ~1, dt0),newdata = data.table(1), times = pd0[, time])
  pd0[["pmf"]] <- c(1, pd0[, surv[-.N]])-pd0[, surv]
#+END_SRC


* Task :noexport:
- [X] References (CV and superlearning +)
- [X] References to Tchetgen and Robins
- [X] Maybe better to call \(\mathcal{E}\) for /algorithms/ (maybe use \(\mathcal{A}\) instead?)
- [ ] Check EIF calculation and that both ways agree
- [X] KL slides
- [ ] Further discussion on the conditional survival function as the target parameter.
- [ ] Add pauses
- [ ] Visualizing results from simulation
- [X] Downplay fixed time horizon 
  
* Setting and data structure
** Data structure and target of inference
\small
# We assume a simple survival setting:
*** Survival setting
- $O = (\tilde T, \Delta, X) \sim P \in \mathcal{P}$ :: Oberved data with $\mathcal{O} = \R_+
  \times \{0,1\} \times \R^p$.
- $Z = (T, X) \sim Q \in \mathcal{Q}$ :: The distribution $Q$ (or a feature of it) is of interest.

# \vfill
  
*** Parameters of interest
- Low-dimensional feature of \(Q\), e.g., the marginal survival probability \(Q(T > t)\) for a fixed
  time horizon \(t \in \R_+\).
- The conditional survival probability \(S(t \mid x) = Q(T > t \mid X=x) \), for \(x \in \R^p\).

\hfill

The distribution \(Q\) is identifiable from the observed data distribution \(P\) under coarsening at
random. Without further assumptions we would typically need to estimate the conditional survival
function \(S\) for both problems.

** Cross-validation and Super Learning for \(S\) 
Most machine learning methods depends on one or more hyperparameters which is typically chosen using
\textbf{cross-validation}.

\vfill

More generally, to build robust estimators we can use \textbf{stacked regression} or
\textbf{Super~Learning} \citep{breiman1996stacked,van2007super} to select from or combine a
collection candidate estimators/algorithms.

\vfill

A central component for both cross-validation and Super Learning is the partitioning of data into
training and test folds. A suitable loss function is then used to evaluate the performance of an
estimator in hold-out samples.

** Evaluate performance in hold-out samples
# Let \(\mathcal{A}\) be a collection of algorithms for estimating \(S \in \mathcal{S}\). Each \(\est
# \in \mathcal{A}\) is a mapping $\mathcal{D} \mapsto \est(\mathcal{D}) = \hat S \in \mathcal{S}$,
# where \(\mathcal{D} = (O_1, \dots, O_n)\) is a data set and $\hat S$ is an estimate of the survival
# function \(S\). Let \(L \colon \mathcal{S} \times \mathcal{O} \rightarrow \R_+\) be a loss
# function.

\small

- \(\mathcal{D}\) :: data set \((O_1, \dots, O_n)\) 
- \(\mathcal{A}\) :: collection of algorithms for estimating \(S \in \mathcal{S}\)
- \(\nu \in \mathcal{A}\) :: mapping $\mathcal{D} \longmapsto \est(\mathcal{D}) = \hat S \in
  \mathcal{S}$
- \(L\) :: loss function, \(L \colon \mathcal{S} \times \mathcal{O} \rightarrow \R_+\)

\vfill
To evaluate the performance of $\est \in \mathcal{A}$ let
- $\mathcal{D}_1, \dots, \mathcal{D}_K$ :: partition of the data set \(\mathcal{D}\)
- \(\mathcal{D}_{-k}\) :: the \(k\)'th training sample, \(\mathcal{D}_{-k} = \mathcal{D} \setminus
  \mathcal{D}_{k}\), \(k=1, \dots, K\)

\vfill For all \(k =1, \dots, K\),
#+begin_export latex
\begin{equation*}
L(\est(\mathcal{D}_{-k}), O_i),
\quad \text{for all} \quad O_i \in \mathcal{D}_k.
\end{equation*}
#+end_export
Averaging these values gives us an estimate of the average loss (risk) of the algorithm \(\nu \in
\mathcal{A}\), and we can then pick the one with lowest risk. Alternatively, we can use these value
to combine all algorithm into a Super Learner.


* Loss functions and hold-out samples for survival data

** The partial likelihood and hold-out samples
A popular choice of loss function for training survival models is the negative partial
log-likelihood. Under coarsening at random and non-informative censoring the likelihood for the
observed data factorizes as
#+begin_export latex
\begin{equation*}
  \ell(P, O) = \ell_t(S, O) \cdot \ell_c(G, O) \cdot \ell_0(\mu, O),
\end{equation*}
where \(G \in \mathcal{G} \) denotes the censoring mechanism and $\mu$ the marginal distribution of
the baseline covariates. The negative partial log-likelihood for the component \(S\) is
\begin{equation*}
  - \log \ell_t(S, O)
  = -
  \left\{
    (1-\Delta) \log S(\tilde T \mid X)
    + \Delta \log f_S(\tilde T \mid X)
  \right\},
\end{equation*}
where \(f_S\) is the conditional density or pmf corresponding to \(S\). \vfill

However, for many common survival estimators this loss function is unsuitable for evaluating
performance in hold-out samples, because (a.s.)
\begin{equation*}
  f_{\hat S}(\tilde T_i \mid X_i) = 0
  \quad \text{when} \quad
  \hat S =\est(\mathcal{D}_{-k})
  \quad \text{and} \quad
  (\tilde T_i, \Delta_i, X_i) \in \mathcal{D}_k.
\end{equation*}
#+end_export

** Illustration
*** base fig
:PROPERTIES:
:BEAMER_act: <1>
:BEAMER_env: onlyenv
:END:


#+BEGIN_SRC R :results graphics file :exports results :file fig-km-surv.pdf :height 5.5
  plot_surv <- ggplot(pd0, aes(x = time, y = surv)) +
    theme_classic() +
    geom_step() +
    xlab("") + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),
		     axis.ticks.x = element_blank(), axis.text.x = element_blank()) +   
    ylab("S") +
    geom_text(data = data.table(time = 1.2, surv = .8,
				text = "survival model trained
  on training sample"),
  aes(label = text))
  plot_surv

  plot_pmf <- ggplot(pd0, aes(x = time)) +
    theme_classic() +
    geom_segment(aes(xend = time, y = 0, yend = pmf), size = 1) +
    xlab("") + theme(axis.ticks.y = element_blank(), axis.text.y = element_blank(),
		     axis.ticks.x = element_blank(), axis.text.x = element_blank()) +
    ylab("pmf") 
  plot_grid(
    plot_surv,
    plot_pmf,
    align = "v", nrow = 2,
    rel_heights = c(2/3, 1/3)
  )
#+END_SRC

#+RESULTS[(2022-08-30 10:39:33) bb9dfedd4d465a6b6e6f03baeadaec40bb3ed276]:
[[file:fig-km-surv.pdf]]

*** fig with dot
:PROPERTIES:
:BEAMER_act: <2>
:BEAMER_env: onlyenv
:END:


#+BEGIN_SRC R :results graphics file :exports results :file fig-km-surv2.pdf  :height 5.5
   plot_grid(
     plot_surv,
     plot_pmf +
     geom_point(data = data.table(time = .9, val = 0), aes(y = val), col = "red", size = 2) +
     geom_text(data = data.table(time = .9, val = .025,
				 text = "hold-out
  sample"),
	       aes(label = text, y = val)),
     align = "v", nrow = 2,
     rel_heights = c(2/3, 1/3)
   )
#+END_SRC

#+RESULTS[(2022-08-30 10:38:33) 188b5c0dab704f5cdec1b13bb1a77bc67496bf91]:
[[file:fig-km-surv2.pdf]]

** Kullback-Leibler divergence and partial likelihoods
#+begin_export latex
\small Maximum likelihood estimation is connected to minimizing the Kullback-Leibler
divergence and gives an interpretation of the MLE under misspecified models.
\begin{equation*}
  \KL(P_0 \, || \, P) := P_0
  {\left[
      % p_1/p_2
    \log \frac{p_0}{p}
  \right]},
  \quad \text{where} \quad
  P_0 = p_0 \cdot \mu,   P = p \cdot \mu.
\end{equation*}
#+end_export

*** misspecified full
:PROPERTIES:
:BEAMER_act: <1>
:BEAMER_env: onlyenv
:END:

#+begin_export latex
\phantom{For a partial likelihood we are minimizing}
\begin{equation*}
 \phantom{  Q \longmapsto \KL(P_{Q_0,G} \, || \, P_{Q,G}), \quad \text{with} \quad Q \in \mathcal{Q}_*.}
\end{equation*}
#+end_export

\vfill

#+begin_export latex
\begin{tikzpicture}
  \draw[line width = .2mm] plot [smooth, tension=.8] coordinates { (0,0) (3,2) (6, 1.2) (9,1)};
  \fill (3,2) circle (0.05);
  \fill (2.6,4) circle (0.05);
  \node[above] (P) at (2.6,4) {\makebox[0pt][l]{$P_0$}\phantom{$P_{Q_0, G}$}};
  \node[] (PP) at (4,.5) {$\mathcal{P}_*$};
  \node[below] (G) at (3,2) {$P_*$};
  \draw[dashed] (3,2) -- (2.6,4);
\end{tikzpicture}
#+end_export

*** misspecified partial 1
:PROPERTIES:
:BEAMER_act: <2>
:BEAMER_env: onlyenv
:END:

#+begin_export latex
For a partial likelihood we are minimizing
\begin{equation*}
  Q \longmapsto \KL(P_{Q_0,G} \, || \, P_{Q,G}),
  \quad \text{with} \quad Q \in \mathcal{Q}_*.
\end{equation*}
#+end_export

\vfill

#+begin_export latex
\begin{tikzpicture}
  \draw[line width = .2mm] plot [smooth, tension=.8] coordinates { (0,0) (3,2) (6, 1.2) (9,1)};
  \fill (3,2) circle (0.05);
  \fill (2.6,4) circle (0.05);
  \node[] (PP) at (4,.5) {$\mathcal{Q}_*$};
  \node[above] (P) at (2.6,4) {$P_{Q_0, G}$};
  \node[below] (G) at (3,2) {$Q_*$};
  \draw[dashed] (2.6,4) -- (3,2);
\end{tikzpicture}
#+end_export

*** misspecifiec partial 2 
:PROPERTIES:
:BEAMER_act: <3>
:BEAMER_env: onlyenv
:END:

#+begin_export latex
For a partial likelihood we are minimizing
\begin{equation*}
  Q \longmapsto \KL(P_{Q_0,G} \, || \, P_{Q,G}),
  \quad \text{with} \quad Q \in \mathcal{Q}_*.
\end{equation*}
#+end_export

\vfill

#+begin_export latex
\begin{tikzpicture}
  \draw[line width = .2mm] plot [smooth, tension=.8] coordinates { (0,0) (3,2) (6, 1.2) (9,1)};
  \node[] (PP) at (4,.5) {$\mathcal{Q}_*$};
  \node[above] (P) at (2.6,4) {$P_{Q_0, G}$};
  \node[above] (P2) at (6.2,3.5) {$P_{Q_0, \tilde{G}}$};
  \node[below] (G) at (3,2) {$Q_*$};
  \node[below] (D) at (6, 1.2) {$\tilde{Q}_*$};
  \draw[dashed] (P2) -- (D);
  \draw[dashed] (2.6,4) -- (3,2);
  \fill (3,2) circle (0.05);
  \fill (2.6,4) circle (0.05);
  \fill (6, 1.2) circle (0.05);
  \fill (6.2,3.5) circle (0.05);
\end{tikzpicture}
#+end_export


** Inverse probability of censoring weighted loss functions

\small A conceptually more attractive strategy is to use loss functions that are
#+ATTR_LATEX: :options [{(i)}]
1. suited for evaluating the performance of estimating the /survival function/
2. defined in terms of the /distribution \(Q\) of interest/

\vfill

We can do this using inverse probability of censoring weighted (IPCW) loss functions. For instance,
with the Brier score
#+begin_export latex
\begin{equation*}
  L_{\mathrm{Brier}}(S, Z) = 
  \left(
    S(t \mid X) - \1
    {\left\{
        T > t
      \right\}}
  \right)^2,
  \quad Z = (T, X) \sim Q,
\end{equation*}
#+end_export
the risk according to this loss is identifiable through
#+begin_export latex
\begin{equation*}
  \E_Q\left[ L_{\mathrm{Brier}}(S, Z) \right]
  = \E_P\left[W_G \cdot L_{\mathrm{Brier}}(S, Z) \right],
\end{equation*}
with
\begin{equation*}
  W_G = \frac{\1{\{\tilde T > t\}} + \1{\{\tilde T \leq t\}}\Delta}{G(\tilde T \wedge t \mid X)},
\end{equation*}
#+end_export
where \(G\) is the conditional ``survivor'' function for the censoring distribution
\citep{graf1999assessment,gerds2006consistent,van2003unicv}.


** Estimation of the IPC weights

\small

To use IPCW loss function in practice we need to estimate the \(G\). This can also be seen as a
survival problem in the sense that the event time of interest is now observed when \(\Delta =0\) and
only partly observed when \(\Delta =1\).

\vfill

$\implies$ The exact same challenges face us when attacking this problem.

# Hence we could use any estimator in \(\mathcal{A}\) and apply it to the data set with observations
# \((\tilde T_i, 1-\Delta_i, X_i)\) to get an estimator of \(G\).

\vfill

#+BEGIN_EXPORT latex
\def\shift{2.3}
\def\ls{}
\def\lw{.5mm}
\begin{center}
\begin{tikzpicture}
  \node[] (S) at (0,\shift) {$\hat S$};
  \node[] (WG) at (\shift,\shift) {$W_{\hat G}$};
  \node[] (G) at (\shift,0) {$\hat G$};
  \node[] (WS) at (0,0) {$W_{\hat S}$};
  \draw[<-, \ls, line width=\lw, cyan] (S) to[out=30,in=150] (WG);
  \draw[<-, \ls, line width=\lw, cyan] (WG) to[out=30-90,in=150-90] (G);
  \draw[<-, \ls, line width=\lw, cyan] (G) to[out=30-180,in=150-180] (WS);
  \draw[<-, \ls, line width=\lw, cyan] (WS) to[out=30-270,in=150-270] (S);
\end{tikzpicture}
\end{center}
#+END_EXPORT

\vfill

Recently, \cite{han2021inverse} and \cite{westling2021inference} have suggested to iterate between
estimation of \(\hat S\) and \(\hat G\) until convergence.


* Double robustness and fluctuation risk
** The conditional survivor function as nuisance parameter
Consider now the situation where we want to estimate a low dimensional feature of \(Q\); as example
we take the marginal survival at a fixed time point, \(Q(T > t)\). Under coarsening at random and a
positivity assumption we can write
#+begin_export latex
\begin{equation*}
  Q(T > t) = \Psi(P),
  \quad \text{where} \quad
  \Psi(P) = \E_P{\left[ S_P(t \mid X) \right]},
\end{equation*}
where \(S_P\) denotes the conditional survival function identifiable from \(P\). 
#+end_export

\vfill

As \(S\) is not of interest in itself, we might hope to side-step the issue of finding a suitable
loss function for the nuisance parameter \(S\) by focusing directly on the target parameter instead.

\vfill

\cite{tchetgenYifanTagetDML}, building on ideas from \cite{robins2007comment}, proposed to exploit
double robustness as a model selection criteria.

** Double robustness
\small Many estimators based on the efficient influence function has a double robustness property.
For instance, the efficient influence function of $\Psi$ is \(\psi(O, P) = \phi(O, S_P, G_P) -
\Psi(P)\), with
#+begin_export latex
\begin{equation*}
  \phi(O, S, G) = S(t \mid X)
  \left(
    1- \int_0^t \frac{N(\diff u) - \1{\{\tilde T \geq u\}} \Lambda_S(\diff u \mid X)}{G(u \mid X) S(u \mid X)}   
  \right),
\end{equation*}
where \(N(u) = \1{\{\tilde T \leq u, \Delta=1\}}\) is the counting process and $\Lambda_S$ is the
conditional cumulative hazard corresponding to \(S\). It holds that
\begin{equation*}
  \E_P{\left[ \phi(O, S_P, G_*) \right]}
  = \E_P{\left[ \phi(O, S_*, G_P) \right]}
  = \Psi(P),
\end{equation*}
for any \(S_*\) and \(G_*\), where \(S_P\) and \(G_P\) are the conditional survivor functions of the
data generating distribution.

\vfill

% With nuisance parameter estimates \(\hat S\) and \(\hat G\) t
This motivates estimating $\Psi(P)$ with
\begin{equation*}
  \hat \Psi = \frac{1}{n}\sum_{i=1}^{n}\phi(O_i, \hat S, \hat G),
\end{equation*}
which is consistent if either \(\hat S\) or \(\hat G\) is consistent.
#+end_export

** Fluctuation risk -- exploiting double robustness
#+begin_export latex
\small Let \(\mathcal{G}\) be a (finite) collection of models for \(G\). The double robustness
property implies that
\(\E_P{\left[ \phi(O, S_P, G) \right]} = \E_P{\left[ \phi(O, S_P, G') \right]}\) for any
\(G, G' \in \mathcal{G}\). In particular,
\begin{equation*}
  \max_{G, G' \in \mathcal{G}}\big\vert
  \E_P{\left[ \phi(O, S_P, G) \right]}
  - \E_P{\left[ \phi(O, S_P, G') \right]}     
  \big\vert
  = 0.
\end{equation*}
This motivates the ``fluctuation risk'',
\begin{equation*}
  % \label{eq:dr}
  R(S)
  =
  \max_{G, G' \in \mathcal{G}}\big\vert
  \E_P{\left[ \phi(O, S, G) \right]}
  - \E_P{\left[ \phi(O, S, G') \right]}     
  \big\vert.
\end{equation*}
% which depend on the class \(\mathcal{G}\).

Let $\mathcal{A}_c$ be a collection of algorithms for estimating \(G\). For any $\nu \in \mathcal{A}$,
$\gamma \in \mathcal{A}_c$, and \(k = 1, \dots, K\) define
\begin{equation*}
  \hat{\Psi}_{\nu, \gamma}^k =
  \frac{1}{|\mathcal{D}_k|} \sum_{O \in \mathcal{D}_k}
  \phi(O, \nu(\mathcal{D}_{-k}), \gamma(\mathcal{D}_{-k})).
\end{equation*}
For any $\nu \in \mathcal{A}$ we approximate the fluctuation risk with
\begin{equation*}
  \hat R(\nu) =
  \frac{1}{K}
  \sum_{k=1}^{K}
  \max_{\gamma, \gamma' \in \mathcal{A}_c}
  \big|\hat{\Psi}_{\nu, \gamma}^k -
  \hat{\Psi}_{\nu, \gamma'}^k\big|.  
\end{equation*}
% and select our final nuisance estimator as
% \begin{equation*}
%   \argmin_{\nu \in \mathcal{A}} R(\nu).
% \end{equation*}
% (We use a similar strategy to pick $\gamma \in \mathcal{A}_c$.)
#+end_export

** Illustration of the method
*** Models
:PROPERTIES:
:BEAMER_act: <1>
:BEAMER_env: onlyenv
:END:

\small Consider the following simple setting where \(X=(A_1, A_2, A_3)^T\) with \(A_j \in \{0,1\}\)
for all \(j\), and that we consider using Kaplan-Meier estimators stratified on each of \(A_j\). (In
this simulation, only \(A_1\) influences survival and censoring.)

\vspace{.4cm}

**** \centering outcome algorithms
#+BEGIN_SRC R :exports code
  km1 <- function(d) prodlim(Surv(time, event) ~ A1, data = d)
  km2 <- function(d) prodlim(Surv(time, event) ~ A2, data = d)
  km3 <- function(d) prodlim(Surv(time, event) ~ A3, data = d)
#+END_SRC

**** \centering censoring algorithms
#+BEGIN_SRC R :exports code
  km1_cens <- function(d) prodlim(Surv(time, !event) ~ A1, data = d)
  km2_cens <- function(d) prodlim(Surv(time, !event) ~ A2, data = d)
  km3_cens <- function(d) prodlim(Surv(time, !event) ~ A3, data = d)
#+END_SRC

*** Figure 
:PROPERTIES:
:BEAMER_act: <2->
:BEAMER_env: onlyenv
:END:

**** overlay block 
:PROPERTIES:
:BEAMER_act: <2>
:BEAMER_env: onlyenv
:END:

#+BEGIN_SRC R :results graphics file :exports results :file "fig-illustrate-fluct-risk.pdf" :width 7 :height 6
  yy0 <- fread(here("jici2022-berkeley", "sim-results", "km-var-selc-illu.txt"))
  yy0[, cens_estimator := paste0(cens_estimator, "_cens")]
  plot_estimates <- ggplot(yy0) +
    ## geom_segment(data = yy0[, .(min_dr = min(dr), max_dr = max(dr)), .(sim, split, out_estimator)],
    ##              aes(x = out_estimator, xend = out_estimator, y = min_dr, yend = max_dr), size = 2, col = "red", alpha = .5) +
    geom_point(position = position_dodge(0.4), aes(x = out_estimator, y = dr, shape = cens_estimator), size = 1.5) +
    theme_bw() +
    facet_grid(paste("Split =", split)~paste("Sim =",sim)) +
    xlab(TeX("")) +
    ylab(TeX("$\\hat{\\Psi}_{\\nu, \\gamma}^k$")) +
    theme(legend.position="top")+
    guides(shape=guide_legend(title=TeX("$\\gamma$ (censoring algorithm)"))) +
    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())
  plot_estimates2 <- ggplot(yy0) +
    geom_segment(data = yy0[, .(min_dr = min(dr), max_dr = max(dr)), .(sim, split, out_estimator)],
		 aes(x = out_estimator, xend = out_estimator, y = min_dr, yend = max_dr), size = 2, col = "red", alpha = .5) +
    geom_point(position = position_dodge(0.4), aes(x = out_estimator, y = dr, shape = cens_estimator), size = 1.5) +
    theme_bw() +
    facet_grid(paste("Split =", split)~paste("Sim =",sim)) +
    xlab(TeX("")) +
    ylab(TeX("$\\hat{\\Psi}_{\\nu, \\gamma}^k$")) +
    theme(legend.position="top")+
    guides(shape=guide_legend(title=TeX("$\\gamma$ (censoring algorithm)"))) +
    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())
  plot_fluct_risk <- ggplot(yy0[, .(range = max(dr)-min(dr)), .(sim, split, out_estimator)][, .(range = mean(range)), .(sim, out_estimator)],
			    aes(x = out_estimator, xend = out_estimator, y = 0, yend = range)) +
    theme_bw() +
    facet_grid("Average range"~sim) +
    theme(strip.text.x = element_blank())+
    ylab(TeX("$\\hat{R}(\\nu)$")) +
    xlab(TeX("$\\nu$ (outcome algorithm)")) +
    ylim(c(0, .16))
  plot_grid(
    plot_estimates,
    plot_fluct_risk,
    align = "v",
    nrow = 2,
    rel_heights = c(2/3, 1/3)
  )
#+END_SRC

#+RESULTS[(2022-08-31 12:47:03) 7a62f9d6839d6ebaa4d1e29f56e26f7fcafa961e]:
[[file:fig-illustrate-fluct-risk.pdf]]

**** overlay block 
:PROPERTIES:
:BEAMER_act: <3>
:BEAMER_env: onlyenv
:END:

#+BEGIN_SRC R :results graphics file :exports results :file "fig-illustrate-fluct-risk2.pdf" :width 7 :height 6
  plot_grid(
    plot_estimates2,
    plot_fluct_risk,
    align = "v",
    nrow = 2,
    rel_heights = c(2/3, 1/3)
  )
#+END_SRC

#+RESULTS[(2022-08-31 12:47:53) 456586e8615e9da8bbe107002859d70f43095064]:
[[file:fig-illustrate-fluct-risk2.pdf]]

**** overlay block 
:PROPERTIES:
:BEAMER_act: <4>
:BEAMER_env: onlyenv
:END:

#+BEGIN_SRC R :results graphics file :exports results :file "fig-illustrate-fluct-risk3.pdf" :width 7 :height 6
  plot_grid(
    plot_estimates2,
    plot_fluct_risk  + geom_segment(size = 2, col = "red", alpha = .5),
    align = "v",
    nrow = 2,
    rel_heights = c(2/3, 1/3)
  )
#+END_SRC

#+RESULTS[(2022-08-31 12:47:59) 9c3768a51c7b9a981852e94ac19468bc5065a6d5]:
[[file:fig-illustrate-fluct-risk3.pdf]]

** Some simulation results

#+BEGIN_SRC R :exports none
  target_est <- fread(here("jici2022-berkeley", "sim-results", "km-var-selc-sim1-est.txt"))
  fluct_info <- fread(here("jici2022-berkeley", "sim-results", "km-var-selc-sim1-fluct-info.txt"))
  plot_target <- target_est[grepl("pre-spec|range",select_criteria) & nuisance_info != "cross-fit"]
  plot_target[,type:=factor(type,levels=c("naive", "dr","fluct"),labels=c("naive", "double robust", "fluctuation"))]
  fluct_select <- fluct_info[, .SD[range == min(range)], .(time, sim, task)][
  , do.call(rbind, lapply(fluct_info[, unique(estimator)], function(ee){
    data.table(estimator = ee, select_n = .SD[estimator == eval(ee), .N])
  })),
  .(time, task)]
  est_plotfun <- function(tt_eval, linesize = 1){
    est_plot = ggplot(plot_target[time == eval(tt_eval)],
		      aes(x = nuisance_info, y = prob)) +
      facet_grid(~type, scales = "free", space = "free_x") + theme_bw() +
      geom_hline(yintercept = target_est[time == eval(tt_eval)][1, true_prob], col = "red", size = linesize) +
      geom_boxplot() +
      ggtitle(paste("Estimated survival probability at time t =", tt_eval)) +
      theme(axis.text.x = element_blank()) +
      xlab("Nuisance estimator(s) used") +
      ylab(TeX("$S(t)$"))
    select_plot = ggplot(fluct_select[task == "out" & time == eval(tt_eval)], aes(x = 1, y = select_n, fill = estimator)) +
      geom_bar(position = "fill", stat = "identity") +
      theme_void() +
      scale_fill_grey() +
      theme(legend.position="bottom") +
      coord_flip() + scale_y_reverse()+
      ggtitle("Outcome algorithm selected by the fluctuation risk")+
      guides(fill=guide_legend(title=""))
    plot_grid(est_plot,
	      select_plot,
	      align = "v",
	      axis = "rl",
	      rel_heights = c(4/5, 1/5),
	      nrow = 2)
  }
#+END_SRC

#+RESULTS[(2022-08-31 14:43:31) 0b1d84e2794c897a2023a54e353b4664ba9d0bc4]:
#+begin_example
       time select_criteria      prob          type nuisance_info sim true_prob
    1:  0.5           range 0.9518191   fluctuation     refit_all   1   0.98210
    2:  0.5        pre-spec 0.9521830         naive           km1   1   0.98210
    3:  0.5        pre-spec 0.9559178         naive           km2   1   0.98210
    4:  0.5        pre-spec 0.9562251         naive           km3   1   0.98210
    5:  0.5        pre-spec 0.9518191 double robust       km1-km1   1   0.98210
   ---                                                                         
19496:  2.5        pre-spec 0.4542094 double robust       km2-km2 300   0.55837
19497:  2.5        pre-spec 0.4533335 double robust       km2-km3 300   0.55837
19498:  2.5        pre-spec 0.4711491 double robust       km3-km1 300   0.55837
19499:  2.5        pre-spec 0.4532741 double robust       km3-km2 300   0.55837
19500:  2.5        pre-spec 0.4520141 double robust       km3-km3 300   0.55837
#+end_example

*** time 1
:PROPERTIES:
:BEAMER_act: <1>
:BEAMER_env: onlyenv
:END:

#+BEGIN_SRC R :results graphics file :exports results :file sim-results1.pdf :width 8 :height 6
est_plotfun(1, linesize=2)
#+END_SRC

#+RESULTS[(2022-08-31 14:39:31) 1278342d2e9f0310a6ee28df5ac04fd431fe558d]:
[[file:sim-results1.pdf]]


*** time 2.5
:PROPERTIES:
:BEAMER_act: <2>
:BEAMER_env: onlyenv
:END:

#+BEGIN_SRC R :results graphics file :exports results :file sim-results2.pdf :width 8 :height 6
est_plotfun(2.5, linesize=2)
#+END_SRC

#+RESULTS[(2022-08-31 14:40:33) 36fe3d48d35d25e5cd5130daa79e52b065ed8ed4]:
[[file:sim-results2.pdf]]

** The conditional survivor function as target parameter
\small Consider now the situation where the conditional survival function \(S(t \mid x)\) is the
actual parameter of interest for fixed \(t\). Assume that our goal is to build a prediction model
minimizing the average Brier score. Given a model \(S \in \mathcal{S}\) we can consider the average
Brier score of \(S\) as a low dimensional target parameter
#+begin_export latex
\begin{equation*}
  \Psi_S(P) = \E_P{\left[ W_G \cdot L_{Brier}(S, Z) \right]}
  \quad \text{with} \quad G = G_P,
\end{equation*}
#+end_export
and proceed as above. \pause

- With a finite /collection/ of models \(\mathcal{S}^* \subset \mathcal{S}\) we get a different target parameter
  $\Psi_S$ for each \(S \in\mathcal{S}^* \). \pause
- With an infinite collection of models \(\mathcal{S}^* \subset \mathcal{S}\) (e.g., indexed by
  \(\beta \in \R^p\)) the previous approach is problematic. \pause

\(\implies\) It is desirable to fit the weights /once/ so that they are "universally"
applicable for estimating the performance of all \(S \in \mathcal{S}\).

\vfill

One idea is to use undersmoothed HAL to do this.

** Conclusion
- It is not obvious what loss function to use for estimating the conditional survivor function with
  censored data observed in continuous time.
- If the parameter of interest is a low-dimension feature of the full data distribution we could
  exploit this and evaluate the performance of the nuisance parameter estimators in terms of their
  effect on the estimator of the target parameter.
- If the conditional survivor function itself is the parameter of interest this approach has some
  additional challenges. 

* References
:PROPERTIES:
:UNNUMBERED: t
:END:
** References
\footnotesize \bibliography{./latex-settings/default-bib.bib}

* HEADER :noexport:
#+TITLE: Loss functions and cross-validation with censored survival data
#+Author: Anders Munch \newline \small joint work with Thomas Gerds
#+Date: September 7, 2022 -- JICI

#+LANGUAGE:  en
#+OPTIONS:   H:2 num:t toc:t ':t ^:t
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [smaller]
#+LaTeX_HEADER: \institute{PhD Student, Section of Biostatistics \\ University of Copenhagen}
#+LaTeX_HEADER: \usepackage{natbib, dsfont, pgfpages, tikz,amssymb, amsmath,xcolor}
#+LaTeX_HEADER: \bibliographystyle{abbrvnat}
#+LaTeX_HEADER: \input{./latex-settings/standard-commands.tex}
#+BIBLIOGRAPHY: ./latex-settings/default-bib plain

#+LaTeX_HEADER: \newcommand{\est}{\ensuremath{\nu}}

# Beamer settins:
# #+LaTeX_HEADER: \usefonttheme[onlymath]{serif} 
#+LaTeX_HEADER: \setbeamertemplate{footline}[frame number]
#+LaTeX_HEADER: \beamertemplatenavigationsymbolsempty
#+LaTeX_HEADER: \usepackage{appendixnumberbeamer}
#+LaTeX_HEADER: \setbeamercolor{gray}{bg=white!90!black}
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+LATEX_HEADER: \setbeamertemplate{itemize items}{$\circ$}

# Check this:
#+LaTeX_HEADER: \lstset{basicstyle=\ttfamily\footnotesize,keywordstyle=\color{black}}

# For handout mode: (check order...)
# #+LATEX_CLASS_OPTIONS: [handout]
# #+LaTeX_HEADER: \pgfpagesuselayout{4 on 1}[border shrink=1mm]
# #+LaTeX_HEADER: \pgfpageslogicalpageoptions{1}{border code=\pgfusepath{stroke}}
# #+LaTeX_HEADER: \pgfpageslogicalpageoptions{2}{border code=\pgfusepath{stroke}}
# #+LaTeX_HEADER: \pgfpageslogicalpageoptions{3}{border code=\pgfusepath{stroke}}
# #+LaTeX_HEADER: \pgfpageslogicalpageoptions{4}{border code=\pgfusepath{stroke}}
