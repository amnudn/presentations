* TODOs                                                            :noexport:
- [X] References
- [X] Lambda and Gamma to plots
- [ ] CV figure
- [X] Definition of super learner -- train and test data
- [ ] Double check product limit definition
- [X] Pauses
  
* Super learning with right-censored data
** Super learning \small (aka cross-validation, stacked regression, ...[fn:1])

\small

\color{bblue}Example: \color{black} Consider estimating a conditional mean \(
f(x) = \E[Y \mid X=x] \) based on data \( \mathcal{D}_n = \{O_1, \dots, O_n\}
\), where \( O_i = (X_i, Y_i) \) are iid.\nbsp{}observations.

\vfill

- Learner :: algorithm \( a \) that produces estimates, \( \mathcal{D}_n \mapsto
  a(\mathcal{D}_n) = \hat f_n \)
- Library :: collection of learners, \( \mathcal{A} = \{a_1, a_2, \dots, a_M \}
  \)
- Loss function :: \( L(a(\mathcal{D}_n), O) \), e.g., \( L(a(\mathcal{D}_n), O)
  = \{a(\mathcal{D}_n)(X) - Y\}^2 \) \pause
- Discrete SL :: \(\hat{a}_n = \argmin_{a\in\mathcal A}\hat{R}_n(a;
  L)\), where
#+begin_export latex
\begin{equation*}
  \hat{R}_n(a; L) =
  \frac{1}{K}\sum_{k=1}^{K}
  \frac{1}{| \mathcal{D}_n^{k} |}\sum_{O_i \in \mathcal{D}_n^{k}}
  L
  {
    \left(
      a{ (\mathcal{D}_n^{-k})}
      , O_i
    \right)
  },
  \quad \text{with} \quad
  \mathcal{D}_n^{-k} = \mathcal{D}_n \setminus \mathcal{D}_n^{k}.
\end{equation*}
#+end_export


\vfill \pause

A super learner can be used for

- model selection and hyperparameter tuning
- stand-alone prediction
- nuisance parameter estimation (e.g., targeted learning of ATE)

[fn:1] \cite{stone1974cross,geisser1975predictive,wolpert1992stacked,breiman1996stacked,van2007super}

** Right-censored data

\small

*** Notation
- \(X\) :: vector of baseline covariates
- \( T \) :: time to event variable, \( T > 0 \)
- \( C \) :: censoring time, \( C > 0 \)
- \color{gray}\(\tilde T \)\color{black} :: censored time to event variable, \(
  \tilde T = \min(T, C) \)
- \color{gray}\( \Delta \)\color{black} :: binary event indicator, \( \Delta =
  \1{\{T \leq C\}} \)
- \color{gray}\( P \)\color{black} :: distribution of the observed data, \( O =
  (X, \tilde T, \Delta) \sim P \)
- \( Q \) :: distribution of the data of interest \( (X, T) \sim Q \)

\hfill

We use \color{bblue} \( \Lambda \) \color{black} and
\color{bblue}\(\Gamma\)\color{black}, respectively, to denote the conditional
cumulative hazard function for \( T \) and \( C \), i.e.,
#+begin_export latex
\begin{equation*}
  \Lambda(\diff t \mid x) = Q(T \in \diff t \mid T \geq t, X=x).
\end{equation*}
#+end_export

We assume \( T \independent C \mid X \) and positivity, which implies that
$\Lambda$ and $\Gamma$ are identifiable from \( P \) on some time interval \(
[0,\tau] \).

** Super learning with right-censored data

\small

- \( P \) :: distribution of the observed data, \( O = (X, \tilde T, \Delta)
  \sim P \)
- \( Q \) :: distribution of the data of interest \( (X, T) \sim Q \)

\hfill

In a survival context, we have data \( \mathcal{D}_n = \{O_1, \dots, O_n\} \)
from \( P \), but we are interested in (a feature of) \( Q \), such as
$\Lambda$.
#+begin_export latex
\begin{equation*}
  \hat{R}_n(a; L) =
  \frac{1}{K}\sum_{k=1}^{K}
  \frac{1}{| \mathcal{D}_n^{k} |}\sum_{O_i \in \mathcal{D}_n^{k}}
  L
  {
    \left(
      a{ (\mathcal{D}_n^{-k})}
      , O_i
    \right)
  },
  \quad \text{with} \quad
  \mathcal{D}_n^{-k} = \mathcal{D}_n \setminus \mathcal{D}_n^{k}.
\end{equation*}
#+end_export

\pause

*** The challenge of censoring

- \( a{ (\mathcal{D}_n^{-k})} \) :: Many learners are available for this type of
  data (e.g., semi-parametric Cox models, parametric survival models,
  (stratified) Kaplan-Meier estimators, random survival forest) \checkmark
- \( L(a{ (\mathcal{D}_n^{-k})} , O_i) \) :: How to evaluate the performance of
  a learner trained in \( \mathcal{D}_n^{-k} \) in the hold-out data \(
  \mathcal{D}_n^{k} \)?

* Existing approaches
** Existing approaches
\small
*** \normalsize Negative log-likelihood loss function \footnotesize (e.g., \cite{polley2011-sl-cens})
Requires discrete time or modeling a Lebesgue hazard function which is
incompatible with many common estimators in survival analysis (e.g.,
Kaplan-Meier, semi-parametric Cox models, and random survival forests).

*** \normalsize Pseudo-observations \footnotesize (e.g., \cite{sachs2019ensemble})
Requires pre-specification of an estimator of the censoring mechanism.

*** \normalsize IPCW \footnotesize (e.g., \cite{hothorn2006survival,gonzalez2021stacked})
Inverse probability of censoring weighted loss functions also require a
pre-specified censoring model.

*** \normalsize Iterative IPCW \footnotesize (\cite{westling2021inference,han2021inverse})

To avoid this, it has been suggested to iterate between estimation of $\Lambda$
and $\Gamma$. No theoretical guarantees for this procedure.

* Proposal: The state learner
** The observed multi-state system
Modeling the conditional state-occupation probabilities of the /observed/ data.

\hfill

*** placeholder
:PROPERTIES:
:BEAMER_act: <1>
:BEAMER_env: onlyenv
:END:

**** \centering \color{white} \( (X, T) \sim Q \)
#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 4 :width 8
plot.new()
#+END_SRC

#+RESULTS[(2023-10-09 22:35:46) 8200075663a0c8102468c8e109f6f3369c0be52e]:
[[file:/tmp/babel-U9iZC3/figure-hAFBVF.pdf]]

*** ideal
:PROPERTIES:
:BEAMER_act: <2>
:BEAMER_env: onlyenv
:END:

**** \centering \( (X, T) \sim Q \)
#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 4 :width 8
  library(prodlim)
  try(setwd("~/Documents/presentations/state-learner/"))
  nTrans <- 1
  stateLabels = c("Initial","Event")
  crHist <- Hist(time = 1:nTrans, event = list(from = rep("1", nTrans), to = stateLabels[-1]))
  plot(crHist,stateLabels = stateLabels,arrowLabels = FALSE, color = "white")
#+END_SRC

#+RESULTS[(2023-10-09 22:34:38) 27df1aecc1dd089a4d0952542a14f8430cb91e23]:
[[file:/tmp/babel-U9iZC3/figure-0XpsvW.pdf]]

*** observed
:PROPERTIES:
:BEAMER_act: <3>
:BEAMER_env: onlyenv
:END:

**** \centering \color{gray}\( (X, \tilde T, \Delta) \sim P \)\color{black}
#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 4 :width 8
nTrans <- 2
stateLabels = c("Initial","Event", "Censored")
crHist <- Hist(time = 1:nTrans, event = list(from = rep("1", nTrans), to = stateLabels[-1]))
plot(crHist,stateLabels = stateLabels,arrowLabels = FALSE)
#+END_SRC

#+RESULTS[(2023-10-09 22:26:19) 9eb322072114fc95ca7bb03d1d6c2c5a9a287dff]:
[[file:/tmp/babel-U9iZC3/figure-W7BHHF.pdf]]


** Conditional state-occupation probabilities for observed data
\small

#+begin_export latex
Record the observed data as \( O = (X, \{\eta(t) : t \geq 0\}) \), where
\begin{equation*}
  \eta(t) = \1{\{\tilde{T} \leq t, \Delta = 1\}} + 2 \, \1{\{\tilde{T} \leq t,
    \Delta = 0\}}
  \in \{0, 1, 2\}.
\end{equation*}

Denote by
\begin{equation*}
  F(t, j, x) = P(\eta(t) = j \mid X=x), \quad \text{for all } t \geq 0,\; j
  \in \{0, 1, 2\}, \; x \in \R^d,
\end{equation*}
the conditional state-occupation probabilities for the observed data.
#+end_export

\vfill

*** multi state 1                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:

\centering \color{bblue}\( O = (X, \tilde T , \Delta) \)\color{black} 

#+ATTR_LATEX: :width 0.9\textwidth
[[./multi-state-data-1.pdf]]

*** multi state 2                                                     :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:

\centering \color{bblue}\( O = (X, \{\eta(t) : t \geq 0\}) \)\color{black}

#+ATTR_LATEX: :width 0.9\textwidth
[[./multi-state-data-3.pdf]]

** The state learner

\small

The state learner builds a super learner for the conditional state-occupation
probabilities,
#+begin_export latex
\begin{equation*}
  F(t, j, x) = P(\eta(t) = j \mid X=x), \quad \text{for all } t \geq 0,\; j
  \in \{0, 1, 2\}, \; x \in \R^d.
\end{equation*}
#+end_export

\( F \) is a feature of the observed data distribution \( P \), so performance
can be evaluated directly as in a "non-survival" setting.

\vfill

#+begin_export latex
We suggest to use the integrated Brier score
\( \bar{B}_{\tau}(F, O) = \int_0^{\tau} B_t(F, O) \diff t \), where
\begin{equation*}
  B_t(F, O) = \sum_{j=0}^{2} (F(t, j, X) - \eta(t))^2.
\end{equation*}
#+end_export

With this choice of loss function no modeling of Lebesgue hazards or densities
is required.

** Expressing \( F \) using $\Lambda$ and $\Gamma$

\small
# The conditional state-occupation probabilites

#+begin_export latex
\begin{equation*}
  F(t, j, x) = P(\eta(t) = j \mid X=x), \quad \text{for all } t \geq 0,\; j
  \in \{0, 1, 2\}, \; x \in \R^d
\end{equation*}
can be expressed (slightly informally) using $\Lambda$ and $\Gamma$,
\begin{equation*}
\begin{split}
F(t, 1, x)
& = P(\tilde{T} \leq t, \Delta=1 \mid X=x)
  = \int_0^t e^{-\Lambda(s \mid x) - \Gamma(s \mid x) }  \Lambda(\diff s \mid x),
\\
F(t, 2, x)
& = P(\tilde{T} \leq t, \Delta=0 \mid X=x)
  = \int_0^t e^{-\Lambda(s \mid x) - \Gamma(s \mid x) }  \Gamma(\diff s \mid x),
\\
F(t, 0, x)
&
  = P(\tilde{T} > t \mid X= x)
  = 1- F(t, 1, x) - F(t, 2, x).
\end{split}
\end{equation*}
#+end_export

\vfill

#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 3 :width 9
nTrans <- 2
stateLabels = c("Initial","Event", "Censored")
crHist <- Hist(time = 1:nTrans, event = list(from = rep("1", nTrans), to = stateLabels[-1]))
plot(crHist,
     stateLabels = stateLabels,
     arrow2.label=paste(expression(Lambda)),
     arrow1.label=paste(expression(Gamma)),
     changeArrowLabelSide=c(FALSE,TRUE))
#+END_SRC

#+RESULTS[(2023-10-10 12:48:34) 323bf458364835310cb3fb2897035ba1fc602791]:
[[file:/tmp/babel-U9iZC3/figure-wHmT7y.pdf]]

** Constructing a library for learning \( F \)

\small

Many learners for $\Lambda$ (and $\Gamma$) are avalaible (Cox models, random
survival forests, etc.).

\vfill

#+begin_export latex
Given libraries \( \mathcal{A} \) and \( \mathcal{B} \) for learning $\Lambda$
and $\Gamma$, respectively,  we construct the library
\begin{equation*}
  \mathcal{F}(\mathcal{A}, \mathcal{B})
  = \{ \phi_{a, b} : a \in \mathcal{A}, b \in \mathcal{B}\},
\end{equation*}
where
\begin{align*}
  \phi_{a, b}(\mathcal{D}_n)(t,1,x) &= \int_0^t e^{-a(\mathcal{D}_n)(s \mid
    x) -
    b(\mathcal{D}_n)(s \mid x) }  a(\mathcal{D}_n)(\diff s \mid x),
  \\
  & \dots 
\end{align*}
#+end_export

#+begin_export latex
We evaluate performance of every
\( \phi_{a, b} \in \mathcal{F}(\mathcal{A}, \mathcal{B}) \) as
\begin{equation*}
  \hat{R}_n(\phi_{a, b}; \bar{B}_{\tau}) =
  \frac{1}{K}\sum_{k=1}^{K}
  \frac{1}{| \mathcal{D}_n^{k} |}\sum_{O_i \in \mathcal{D}_n^{k}}
  \int_0^{\tau} \sum_{j=0}^{2} 
  \left\{
    \phi_{a, b}(\mathcal{D}_n^{-k})(t,j, X_i) - \eta_i(t)
  \right\}^2 \diff t.
\end{equation*}
#+end_export


** Some theoretical results :noexport:

\small Write \( P_0 \) for the "true" data-generating distribution, and \(
P_0{[f]} = \int f(o) P(\diff o) \).

\vfill

- Discrete SL :: \( \hat{\phi}_n = \argmin_{(a,b) \in \mathcal{A} \times
  \mathcal{B}} \frac{1}{K}\sum_{k=1}^K
  \mathbb{P}_n^k{[\bar{B}_{\tau}(\phi_{a,b}(\mathcal{D}_n^{-k}), \blank)]} \),
  where \( \mathbb{P}_n^k \) is the empirical measure of \( \mathcal{D}_n^k \)
- Oracle :: \( \tilde{\phi}_n = \argmin_{(a,b) \in \mathcal{A} \times
  \mathcal{B}} \frac{1}{K}\sum_{k=1}^K
  P_0{[\bar{B}_{\tau}(\phi_{a,b}(\mathcal{D}_n^{-k}), \blank)]} \)
- True risk minimizer :: \( F^* = \argmin_F P_0{[\bar{B}_{\tau}(F, \blank)]} \)

\vfill

\( F^* = F_0 \) where \( F_0 \) is the conditional state-occupation probability
function corresponding to \( P_0 \).

\vfill

Using results from \citep{van2003unicv,van2006oracle} we can establish a finite
sample oracle inequality for the state learner.

\vfill

Asymptotic consequence: If \( |\mathcal{F}(\mathcal{A}_n,\mathcal{B}_n)| =
O(n^q) \), for some \( q \in \N \), and the library contains a learner that
converges to \( F_0 \) at rate \( r_n \), then the state learner converges to
\(F_0 \) at the same rate or at rate \( \log(n) r_n \).

** Almost minimum viable product

\footnotesize

#+BEGIN_SRC R :results silent
  library(targets)
  library(data.table)
  tar_load(zelefsky, store = "~/Documents/phd/survival-loss-function/statelearner/empirical-study/_targets/")
  zelefsky[, time := dmos]
  use_dat = copy(zelefsky)[, .(time,status,logPSA,stage,ggtot,sDose,hormones,vital)]
  use_dat[status == 0 & vital == "Dead", status := 2][, vital := NULL]
  tar_load(zelefsky_statelearner_real_data_comp, store = "~/Documents/phd/survival-loss-function/statelearner/empirical-study/_targets/")
  head(zelefsky_statelearner_real_data_comp)
#+END_SRC

#+BEGIN_SRC R :exports code :results silent
  head(use_dat, n=4)
#+END_SRC

#+BEGIN_SRC R
  use_dat[c(1:3, 9), ]
#+END_SRC

#+RESULTS[(2023-10-10 15:30:13) bc6ea71a25a022a5ca626233554871fcade89a12]:
:        time status   logPSA stage ggtot      sDose hormones
: 1: 30.78737      0 1.791759   T1c     6  0.1663670       No
: 2: 28.69895      0 2.468100   T3c     9  0.1663670      Yes
: 3: 11.99158      0 3.086487   T1c     3 -0.9372808       No
: 4: 38.13053      1 2.890372   T1c     6 -0.9372808       No

#+BEGIN_SRC R :results silent :eval never :exports code
library <- list(
  cox_lasso = list("GLMnet"),
  cox_elastic = list("GLMnet", alpha = 0.5),
  rf = list("rfsrc", ntree = 500))
fit_sl <- statelearner(
  list(cause1 = library, censor = library),
  data = use_dat, time = 36),
head(fit_sl, n=4)
#+END_SRC

#+BEGIN_SRC R
  head(zelefsky_statelearner_real_data_comp, n=4)[, -2]
#+END_SRC

#+RESULTS[(2023-10-10 16:19:54) 712c4cefddd56fa5d0e3700d9252c4e5a46dbbcd]:
:         cause1 censor     loss         sd
: 1: cox_elastic     rf 7.034702 0.02159417
: 2: cox_elastic     rf 7.034812 0.02286074
: 3:   cox_lasso     rf 7.035051 0.02142064
: 4:   cox_lasso     rf 7.035231 0.02266556



** Some theoretical results

\small

*** Finite sample guarantee

Using results from \citep{van2003unicv,van2006oracle} we can establish a finite
sample oracle inequality for the state learner.

\hfill

This means that the state learner will perform almost as well as a so-called
"oracle" which uses the unknown data-generating distribution to evaluate
performance of the learners.

\hfill

*** Asymptotic consequence
Let \( F_0 \) denote the conditional state-occupation probability function
corresponding to the underlying data-generating distribution \( P_0 \). If

- \( |\mathcal{F}(\mathcal{A}_n,\mathcal{B}_n)| = O(n^q) \), for some \( q \in
  \N \), and
- the library contains a learner that converges to \( F_0 \) at rate \( r_n \),

then the state learner converges to \(F_0 \) at the same rate or at rate \(
\log(n) r_n \).


** Proof of concept -- simulation I
\small

- Univariate \( X \)
- Cox model and the Nelson-Aalen estimator in the libraries
- Compare to IPCW weighted estimators using wrongly (IPCW(KM)) and correctly
  (IPCW(Cox)) specified censoring models
- Evaluate performance of survival predictions at fixed prediction horizon

#+ATTR_LATEX: :width .9\textwidth
[[./ipcw-fail.pdf]]

** Proof of concept -- simulation II
\small

- Multivariate \( X \)
- Several strong learners: Cox models (various stratifications and splines),
  penalized Cox models (lasso, ridge, elastic), random survival forest
- Data generated according to a simulation of a prostate cancer study
  \citep{kattan2000pretreatment,gerds2013estimating}.

#+ATTR_LATEX: :width .9\textwidth
[[./zelefski-sim.pdf]]

** Competing risks
\small

# The state learner can be adapted to competing risk settings by adding a state:

#+BEGIN_SRC R :results graphics file :exports results :file (org-babel-temp-file "./figure-" ".pdf") :height 3 :width 9
  nTrans <- 3
  stateLabels = c("Initial","Cause 1","Cause 2", "Censored")
  crHist <- Hist(time = 1:nTrans, event = list(from = rep("1", nTrans), to = stateLabels[-1]))
  plot(crHist,
       stateLabels = stateLabels,
       arrow1.label=paste(expression(Lambda[1])),
       arrow2.label=paste(expression(Lambda[2])),
       arrow3.label=paste(expression(Gamma)),
       changeArrowLabelSide=c(TRUE, TRUE, FALSE))
#+END_SRC

#+RESULTS[(2023-10-10 14:08:45) 3cd681f19bf672afb59bb96d0401a214522c2a7b]:
[[file:/tmp/babel-U9iZC3/figure-jHSzxz.pdf]]

#+begin_export latex
\begin{equation*}
  \eta(t) = \1{\{\tilde{T} \leq t, \tilde{D} = 1\}} +
  2 \, \1{\{\tilde{T} \leq t, \tilde{D} = 2\}}
  +
  3 \, \1{\{\tilde{T} \leq t, \tilde{D} = 0\}}.
\end{equation*}
\begin{align*}
  F(t, 1, x)
  & = P(\tilde{T} \leq t, \tilde{D}=1 \mid X=x)
    = \int_0^t e^{-\Lambda_1(s \mid x) -\Lambda_2(s \mid x) - \Gamma(s \mid x) }  \Lambda_1(\diff s \mid x),
  \\
  & \dots
\end{align*}
\begin{equation*}
  \mathcal{F}(\mathcal{A}_1,\mathcal{A}_2, \mathcal{B})
  = \{ \phi_{a_1, a_2, b} : a_1 \in \mathcal{A}_1, a_2 \in \mathcal{A}_2, b \in \mathcal{B}\},
\end{equation*}
#+end_export

** Proof of concept -- some real data
\small The real data considered in \citep{kattan2000pretreatment} included the
competing risk of death.

#+ATTR_LATEX: :width 1\textwidth
[[./zelefski-real-data.pdf]]

* Discussion
** Discussion
\small

A clear limitation is that the function \( F \) is typically not a parameter of
interest.

\vfill

We can obtain a risk prediction model from the state learner using that
#+begin_export latex
\begin{equation*}
  \Lambda(t \mid x) = \int_0^t \frac{F(\diff s, 1, x)}{F(s-, 0, x)} ,
  \quad \text{and} \quad
  S(t \mid x)
  % = Q(T > t \mid X=x)
  % = e^{\Lambda(t \mid x)}
  = \prodi_{s \leq t}(1- \Lambda(\diff s \mid x)).
\end{equation*}
#+end_export

However, the state learner does not evaluate the learners based on their risk
prediction performances but on how well a tuple \( (\Lambda, \Gamma) \) of
learners jointly model the observed data.

\vfill

When estimating low-dimensional target parameter and the state learner is used
to estimate the nuisance parameters, this is probably less of a concern.

\vfill

Unclear if the state learner will respond well to positivity violations or not.
  
** Conclusion

\small

- To avoid the need to pre-specify a censoring model, we propose to use learners
  for $\Lambda$ and $\Gamma$ to jointly model the observed data.
- We select a tuple of learners \( (\Lambda, \Gamma) \) that is jointly optimal
  for predicting the states occupied by the observed data conditional on
  baseline covariates.
- We use the integrated Brier score to evaluate performance with
  respect to the observed data distribution.
- No need to model additional nuisance parameters to estimate performance in
  hold-out samples.
- No need to estimate Lebesgue densities or hazards.
- Drawback is that the SL is tuned for the a feature of the observed
  distribution \( P \) and not for a feature of \( Q \).

\vfill


*** Questions, comments, suggestions?

    \vfill

    \flushright Thank you for listening!

* References
:PROPERTIES:
:UNNUMBERED: t
:END:
** References
\tiny \bibliography{./latex-settings/default-bib.bib}

* Setting :noexport:
Remember to exceture (C-c C-c) the following line:
#+PROPERTY: header-args:R :async :results output verbatim  :exports results  :session *R* :cache yes

* HEADER :noexport:
#+TITLE: The state learner \newline \normalsize a super learner for right-censored data
#+Author: Anders Munch \newline \small joint work with Thomas Gerds
#+Date: \today

#+LANGUAGE:  en
#+OPTIONS:   H:2 num:t toc:t ':t ^:t
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [smaller]
#+LaTeX_HEADER: \usepackage{natbib, dsfont, pgfpages, tikz,amssymb, amsmath,xcolor}
#+LaTeX_HEADER: \bibliographystyle{abbrvnat}
#+LaTeX_HEADER: \input{./latex-settings/standard-commands.tex}
#+BIBLIOGRAPHY: ./latex-settings/default-bib plain

#+LaTeX_HEADER: \definecolor{bblue}{rgb}{0.2,0.2,0.7}
#+LaTeX_HEADER: \usepackage{prodint}

# Beamer settins:
# #+LaTeX_HEADER: \usefonttheme[onlymath]{serif} 
#+LaTeX_HEADER: \setbeamertemplate{footline}[frame number]
#+LaTeX_HEADER: \beamertemplatenavigationsymbolsempty
#+LaTeX_HEADER: \usepackage{appendixnumberbeamer}
#+LaTeX_HEADER: \setbeamercolor{gray}{bg=white!90!black}
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+LATEX_HEADER: \setbeamertemplate{itemize items}{$\circ$}

# Check this:
#+LaTeX_HEADER: \lstset{basicstyle=\ttfamily\footnotesize}

# For handout mode: (check order...)
# #+LATEX_CLASS_OPTIONS: [handout]
# #+LaTeX_HEADER: \pgfpagesuselayout{4 on 1}[border shrink=1mm]
# #+LaTeX_HEADER: \pgfpageslogicalpageoptions{1}{border code=\pgfusepath{stroke}}
# #+LaTeX_HEADER: \pgfpageslogicalpageoptions{2}{border code=\pgfusepath{stroke}}
# #+LaTeX_HEADER: \pgfpageslogicalpageoptions{3}{border code=\pgfusepath{stroke}}
# #+LaTeX_HEADER: \pgfpageslogicalpageoptions{4}{border code=\pgfusepath{stroke}}
